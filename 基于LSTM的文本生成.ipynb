{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75faaa9d-e5b4-4103-9c30-88da6b84ba17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'欲出未出光辣达，千山万山如火发。须臾走向天上来，逐却残星赶却月。'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "data_path = \"./data/data\"\n",
    "batch_size = 8\n",
    "final_model_path = \"./model/Poetry_LSTM_model.pt\"\n",
    "data = load_from_disk(data_path)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "poetry = data['poetry']\n",
    "poetry[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac22d4e-a15c-46d9-a202-e47cda086474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表长度:9050\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# 加载词典\n",
    "def load_vocab(vocab_path):\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "        vocab = json.load(f)\n",
    "    return vocab\n",
    "\n",
    "# 加载词汇表\n",
    "vocab_path = \"./data/vocab.json\"\n",
    "vocab = load_vocab(vocab_path)\n",
    "print(f\"词汇表长度:{len(vocab['char_to_id'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835ea17f-98d2-40ff-b68f-97ba0fc217c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "# 最长的诗词\n",
    "max_len = max(len(s) for s in poetry)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e87fe9d-22b9-4117-9e5d-486a0a3bf49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, vocab, max_length=48):\n",
    "        self.max_length = max_length\n",
    "        self.char_to_id = vocab['char_to_id']\n",
    "        self.id_to_char = vocab['id_to_char']\n",
    "\n",
    "        # 将输入文本和标签转换为ID形式\n",
    "        datas = [self.text_to_id(text) for text in dataset]\n",
    "        self.inputs = [[self.char_to_id['<SOS>']] + ids for ids in datas]\n",
    "        self.labels = [ids + [self.char_to_id['<EOS>']] for ids in datas]\n",
    "\n",
    "        # 对输入进行填充\n",
    "        self.inputs = self.pad_sequence(self.inputs, pad_value=self.char_to_id['<PAD>'])\n",
    "        self.labels = self.pad_sequence(self.labels, pad_value=self.char_to_id['<PAD>'])  # 使用 0 标签作为填充值\n",
    "\n",
    "    def text_to_id(self, text):\n",
    "        \"\"\"将字符序列转换为词汇表中的ID\"\"\"\n",
    "        return [self.char_to_id.get(char, self.char_to_id['<UNK>']) for char in text]\n",
    "\n",
    "    def pad_sequence(self, sequences, pad_value=0):\n",
    "        \"\"\"填充序列到最大长度\"\"\"\n",
    "        return [\n",
    "            seq + [pad_value] * (self.max_length - len(seq)) if len(seq) < self.max_length else seq[:self.max_length]\n",
    "            for seq in sequences]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.inputs[idx])\n",
    "        label_ids = torch.tensor(self.labels[idx])\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': label_ids\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(poetry, vocab, max_len + 2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6d14c0-9533-4c07-b393-b8002fde7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# LSTM 语言模型\n",
    "class LSTMTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=256):\n",
    "        super(LSTMTextGenerator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 2\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=vocab['char_to_id']['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.shape[0]\n",
    "        if hidden is None:\n",
    "            h0 = torch.tensor(np.zeros((self.num_layers, batch_size, self.hidden_size), dtype=np.float32)).to(device)\n",
    "            c0 = torch.tensor(np.zeros((self.num_layers, batch_size, self.hidden_size), dtype=np.float32)).to(device)\n",
    "        else:\n",
    "            h0, c0 = hidden\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64432957-347f-40a3-af56-4578c7c64dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现模型文件 ./model/Poetry_LSTM_model.pt，正在加载...\n",
      "模型加载完成\n",
      "\tBatch 0/22849, Loss: 4.8245\n",
      "\tBatch 500/22849, Loss: 4.7323\n",
      "\tBatch 1000/22849, Loss: 5.0995\n",
      "\tBatch 1500/22849, Loss: 4.8943\n",
      "\tBatch 2000/22849, Loss: 4.7676\n",
      "\tBatch 2500/22849, Loss: 5.1930\n",
      "\tBatch 3000/22849, Loss: 4.7066\n",
      "\tBatch 3500/22849, Loss: 5.0375\n",
      "\tBatch 4000/22849, Loss: 4.9793\n",
      "\tBatch 4500/22849, Loss: 5.0903\n",
      "\tBatch 5000/22849, Loss: 4.6616\n",
      "\tBatch 5500/22849, Loss: 4.5518\n",
      "\tBatch 6000/22849, Loss: 4.6911\n",
      "\tBatch 6500/22849, Loss: 4.9674\n",
      "\tBatch 7000/22849, Loss: 4.5144\n",
      "\tBatch 7500/22849, Loss: 4.8321\n",
      "\tBatch 8000/22849, Loss: 4.4702\n",
      "\tBatch 8500/22849, Loss: 4.6591\n",
      "\tBatch 9000/22849, Loss: 4.9835\n",
      "\tBatch 9500/22849, Loss: 4.6525\n",
      "\tBatch 10000/22849, Loss: 4.6228\n",
      "\tBatch 10500/22849, Loss: 4.8135\n",
      "\tBatch 11000/22849, Loss: 4.9328\n",
      "\tBatch 11500/22849, Loss: 4.9268\n",
      "\tBatch 12000/22849, Loss: 4.9881\n",
      "\tBatch 12500/22849, Loss: 4.9302\n",
      "\tBatch 13000/22849, Loss: 4.5632\n",
      "\tBatch 13500/22849, Loss: 4.7830\n",
      "\tBatch 14000/22849, Loss: 4.8030\n",
      "\tBatch 14500/22849, Loss: 4.6546\n",
      "\tBatch 15000/22849, Loss: 4.7703\n",
      "\tBatch 15500/22849, Loss: 4.7135\n",
      "\tBatch 16000/22849, Loss: 4.7048\n",
      "\tBatch 16500/22849, Loss: 4.8637\n",
      "\tBatch 17000/22849, Loss: 4.9528\n",
      "\tBatch 17500/22849, Loss: 4.8317\n",
      "\tBatch 18000/22849, Loss: 5.1022\n",
      "\tBatch 18500/22849, Loss: 4.8387\n",
      "\tBatch 19000/22849, Loss: 5.2105\n",
      "\tBatch 19500/22849, Loss: 4.6282\n",
      "\tBatch 20000/22849, Loss: 4.7120\n",
      "\tBatch 20500/22849, Loss: 4.4704\n",
      "\tBatch 21000/22849, Loss: 4.6124\n",
      "\tBatch 21500/22849, Loss: 4.4849\n",
      "\tBatch 22000/22849, Loss: 5.3574\n",
      "\tBatch 22500/22849, Loss: 4.7192\n",
      "Epoch 1/5, Loss: 4.8064\n",
      "\tBatch 0/22849, Loss: 4.6879\n",
      "\tBatch 500/22849, Loss: 4.6871\n",
      "\tBatch 1000/22849, Loss: 4.4685\n",
      "\tBatch 1500/22849, Loss: 4.6635\n",
      "\tBatch 2000/22849, Loss: 4.6063\n",
      "\tBatch 2500/22849, Loss: 4.8296\n",
      "\tBatch 3000/22849, Loss: 4.2343\n",
      "\tBatch 3500/22849, Loss: 4.5607\n",
      "\tBatch 4000/22849, Loss: 4.5034\n",
      "\tBatch 4500/22849, Loss: 4.6822\n",
      "\tBatch 5000/22849, Loss: 4.5984\n",
      "\tBatch 5500/22849, Loss: 4.3777\n",
      "\tBatch 6000/22849, Loss: 4.4230\n",
      "\tBatch 6500/22849, Loss: 4.9977\n",
      "\tBatch 7000/22849, Loss: 4.1677\n",
      "\tBatch 7500/22849, Loss: 4.4373\n",
      "\tBatch 8000/22849, Loss: 4.4557\n",
      "\tBatch 8500/22849, Loss: 4.7345\n",
      "\tBatch 9000/22849, Loss: 4.2694\n",
      "\tBatch 9500/22849, Loss: 4.3973\n",
      "\tBatch 10000/22849, Loss: 4.7455\n",
      "\tBatch 10500/22849, Loss: 4.5588\n",
      "\tBatch 11000/22849, Loss: 4.5405\n",
      "\tBatch 11500/22849, Loss: 4.6002\n",
      "\tBatch 12000/22849, Loss: 4.3364\n",
      "\tBatch 12500/22849, Loss: 4.5647\n",
      "\tBatch 13000/22849, Loss: 4.3506\n",
      "\tBatch 13500/22849, Loss: 4.7471\n",
      "\tBatch 14000/22849, Loss: 4.2579\n",
      "\tBatch 14500/22849, Loss: 4.4671\n",
      "\tBatch 15000/22849, Loss: 4.4874\n",
      "\tBatch 15500/22849, Loss: 4.4020\n",
      "\tBatch 16000/22849, Loss: 4.6021\n",
      "\tBatch 16500/22849, Loss: 4.7479\n",
      "\tBatch 17000/22849, Loss: 4.2654\n",
      "\tBatch 17500/22849, Loss: 4.7028\n",
      "\tBatch 18000/22849, Loss: 4.4611\n",
      "\tBatch 18500/22849, Loss: 4.7025\n",
      "\tBatch 19000/22849, Loss: 4.6744\n",
      "\tBatch 19500/22849, Loss: 4.1573\n",
      "\tBatch 20000/22849, Loss: 4.4771\n",
      "\tBatch 20500/22849, Loss: 4.3875\n",
      "\tBatch 21000/22849, Loss: 4.2242\n",
      "\tBatch 21500/22849, Loss: 4.4267\n",
      "\tBatch 22000/22849, Loss: 4.6958\n",
      "\tBatch 22500/22849, Loss: 4.4827\n",
      "Epoch 2/5, Loss: 4.5278\n",
      "\tBatch 0/22849, Loss: 4.2983\n",
      "\tBatch 500/22849, Loss: 4.0861\n",
      "\tBatch 1000/22849, Loss: 4.5551\n",
      "\tBatch 1500/22849, Loss: 4.5554\n",
      "\tBatch 2000/22849, Loss: 4.1393\n",
      "\tBatch 2500/22849, Loss: 4.3833\n",
      "\tBatch 3000/22849, Loss: 4.3684\n",
      "\tBatch 3500/22849, Loss: 4.5532\n",
      "\tBatch 4000/22849, Loss: 4.5701\n",
      "\tBatch 4500/22849, Loss: 4.1839\n",
      "\tBatch 5000/22849, Loss: 4.4607\n",
      "\tBatch 5500/22849, Loss: 4.3255\n",
      "\tBatch 6000/22849, Loss: 4.1933\n",
      "\tBatch 6500/22849, Loss: 4.1301\n",
      "\tBatch 7000/22849, Loss: 4.0463\n",
      "\tBatch 7500/22849, Loss: 4.5591\n",
      "\tBatch 8000/22849, Loss: 4.4062\n",
      "\tBatch 8500/22849, Loss: 4.5370\n",
      "\tBatch 9000/22849, Loss: 4.4616\n",
      "\tBatch 9500/22849, Loss: 4.6483\n",
      "\tBatch 10000/22849, Loss: 4.4586\n",
      "\tBatch 10500/22849, Loss: 4.2819\n",
      "\tBatch 11000/22849, Loss: 4.3915\n",
      "\tBatch 11500/22849, Loss: 4.5686\n",
      "\tBatch 12000/22849, Loss: 4.3263\n",
      "\tBatch 12500/22849, Loss: 4.5698\n",
      "\tBatch 13000/22849, Loss: 4.6019\n",
      "\tBatch 13500/22849, Loss: 4.3687\n",
      "\tBatch 14000/22849, Loss: 4.1005\n",
      "\tBatch 14500/22849, Loss: 4.5744\n",
      "\tBatch 15000/22849, Loss: 4.4890\n",
      "\tBatch 15500/22849, Loss: 4.4370\n",
      "\tBatch 16000/22849, Loss: 4.4913\n",
      "\tBatch 16500/22849, Loss: 4.2464\n",
      "\tBatch 17000/22849, Loss: 4.6987\n",
      "\tBatch 17500/22849, Loss: 4.3729\n",
      "\tBatch 18000/22849, Loss: 4.1933\n",
      "\tBatch 18500/22849, Loss: 4.3164\n",
      "\tBatch 19000/22849, Loss: 5.0362\n",
      "\tBatch 19500/22849, Loss: 4.7464\n",
      "\tBatch 20000/22849, Loss: 4.1673\n",
      "\tBatch 20500/22849, Loss: 4.0697\n",
      "\tBatch 21000/22849, Loss: 4.3808\n",
      "\tBatch 21500/22849, Loss: 4.4408\n",
      "\tBatch 22000/22849, Loss: 4.4857\n",
      "\tBatch 22500/22849, Loss: 4.4685\n",
      "Epoch 3/5, Loss: 4.3909\n",
      "\tBatch 0/22849, Loss: 4.6001\n",
      "\tBatch 500/22849, Loss: 4.3808\n",
      "\tBatch 1000/22849, Loss: 4.2672\n",
      "\tBatch 1500/22849, Loss: 4.6085\n",
      "\tBatch 2000/22849, Loss: 4.1660\n",
      "\tBatch 2500/22849, Loss: 4.6198\n",
      "\tBatch 3000/22849, Loss: 4.2152\n",
      "\tBatch 3500/22849, Loss: 4.1542\n",
      "\tBatch 4000/22849, Loss: 4.6654\n",
      "\tBatch 4500/22849, Loss: 4.2862\n",
      "\tBatch 5000/22849, Loss: 4.6079\n",
      "\tBatch 5500/22849, Loss: 4.5164\n",
      "\tBatch 6000/22849, Loss: 4.1697\n",
      "\tBatch 6500/22849, Loss: 4.4863\n",
      "\tBatch 7000/22849, Loss: 4.3056\n",
      "\tBatch 7500/22849, Loss: 4.6984\n",
      "\tBatch 8000/22849, Loss: 3.9812\n",
      "\tBatch 8500/22849, Loss: 4.3835\n",
      "\tBatch 9000/22849, Loss: 4.0827\n",
      "\tBatch 9500/22849, Loss: 4.2459\n",
      "\tBatch 10000/22849, Loss: 4.2483\n",
      "\tBatch 10500/22849, Loss: 4.7054\n",
      "\tBatch 11000/22849, Loss: 4.1710\n",
      "\tBatch 11500/22849, Loss: 4.2780\n",
      "\tBatch 12000/22849, Loss: 3.7388\n",
      "\tBatch 12500/22849, Loss: 4.2412\n",
      "\tBatch 13000/22849, Loss: 4.6042\n",
      "\tBatch 13500/22849, Loss: 4.1153\n",
      "\tBatch 14000/22849, Loss: 4.2900\n",
      "\tBatch 14500/22849, Loss: 4.6077\n",
      "\tBatch 15000/22849, Loss: 4.2443\n",
      "\tBatch 15500/22849, Loss: 4.4185\n",
      "\tBatch 16000/22849, Loss: 4.3363\n",
      "\tBatch 16500/22849, Loss: 4.3798\n",
      "\tBatch 17000/22849, Loss: 4.2009\n",
      "\tBatch 17500/22849, Loss: 4.1554\n",
      "\tBatch 18000/22849, Loss: 4.3370\n",
      "\tBatch 18500/22849, Loss: 4.2778\n",
      "\tBatch 19000/22849, Loss: 4.2590\n",
      "\tBatch 19500/22849, Loss: 4.0836\n",
      "\tBatch 20000/22849, Loss: 4.2091\n",
      "\tBatch 20500/22849, Loss: 4.1212\n",
      "\tBatch 21000/22849, Loss: 4.4880\n",
      "\tBatch 21500/22849, Loss: 4.3398\n",
      "\tBatch 22000/22849, Loss: 4.0829\n",
      "\tBatch 22500/22849, Loss: 4.1633\n",
      "Epoch 4/5, Loss: 4.3035\n",
      "\tBatch 0/22849, Loss: 3.9909\n",
      "\tBatch 500/22849, Loss: 4.0794\n",
      "\tBatch 1000/22849, Loss: 4.1710\n",
      "\tBatch 1500/22849, Loss: 3.9395\n",
      "\tBatch 2000/22849, Loss: 4.0552\n",
      "\tBatch 2500/22849, Loss: 4.1667\n",
      "\tBatch 3000/22849, Loss: 4.1319\n",
      "\tBatch 3500/22849, Loss: 4.0149\n",
      "\tBatch 4000/22849, Loss: 4.5207\n",
      "\tBatch 4500/22849, Loss: 3.9918\n",
      "\tBatch 5000/22849, Loss: 4.3303\n",
      "\tBatch 5500/22849, Loss: 4.2763\n",
      "\tBatch 6000/22849, Loss: 4.1622\n",
      "\tBatch 6500/22849, Loss: 4.0719\n",
      "\tBatch 7000/22849, Loss: 4.0790\n",
      "\tBatch 7500/22849, Loss: 4.4028\n",
      "\tBatch 8000/22849, Loss: 4.0376\n",
      "\tBatch 8500/22849, Loss: 4.0310\n",
      "\tBatch 9000/22849, Loss: 4.3624\n",
      "\tBatch 9500/22849, Loss: 4.1719\n",
      "\tBatch 10000/22849, Loss: 4.3322\n",
      "\tBatch 10500/22849, Loss: 4.2173\n",
      "\tBatch 11000/22849, Loss: 4.4052\n",
      "\tBatch 11500/22849, Loss: 4.4156\n",
      "\tBatch 12000/22849, Loss: 4.1868\n",
      "\tBatch 12500/22849, Loss: 3.9775\n",
      "\tBatch 13000/22849, Loss: 4.0435\n",
      "\tBatch 13500/22849, Loss: 3.8091\n",
      "\tBatch 14000/22849, Loss: 4.1803\n",
      "\tBatch 14500/22849, Loss: 4.0613\n",
      "\tBatch 15000/22849, Loss: 4.2253\n",
      "\tBatch 15500/22849, Loss: 4.2608\n",
      "\tBatch 16000/22849, Loss: 4.4652\n",
      "\tBatch 16500/22849, Loss: 4.1564\n",
      "\tBatch 17000/22849, Loss: 4.4628\n",
      "\tBatch 17500/22849, Loss: 4.2699\n",
      "\tBatch 18000/22849, Loss: 4.1604\n",
      "\tBatch 18500/22849, Loss: 4.3125\n",
      "\tBatch 19000/22849, Loss: 4.3912\n",
      "\tBatch 19500/22849, Loss: 4.3510\n",
      "\tBatch 20000/22849, Loss: 4.0183\n",
      "\tBatch 20500/22849, Loss: 4.3387\n",
      "\tBatch 21000/22849, Loss: 4.2145\n",
      "\tBatch 21500/22849, Loss: 3.8631\n",
      "\tBatch 22000/22849, Loss: 4.2109\n",
      "\tBatch 22500/22849, Loss: 4.0769\n",
      "Epoch 5/5, Loss: 4.2446\n"
     ]
    }
   ],
   "source": [
    "# 训练和生成函数（简化版）\n",
    "import os\n",
    "def train_lstm_model():\n",
    "    model = LSTMTextGenerator(len(vocab['char_to_id'])).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    epoches = 20\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab['char_to_id']['<PAD>'])  # 忽略填充值\n",
    "    # 检查模型是否已经存在\n",
    "    if os.path.isfile(final_model_path):\n",
    "        print(f\"发现模型文件 {final_model_path}，正在加载...\")\n",
    "        model.load_state_dict(torch.load(final_model_path))\n",
    "        print(\"模型加载完成\")\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(epoches):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for num, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(input_ids)\n",
    "            # 调整输出和标签的形状\n",
    "            output = output.view(-1, len(vocab['char_to_id']))  # (batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(-1)  # (batch_size * seq_len)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if num % 500 == 0:\n",
    "                print(f\"\\tBatch {num}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}/{epoches}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "\n",
    "train_lstm_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
